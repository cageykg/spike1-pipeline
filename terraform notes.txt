

Concerns
Running before we can walk - feels like the early days of chef. The tool is easy to use as a single user. very easy to get up and running.
  * large team concerns
  * varying change velocity per stacks
  * catering for manually built environments
Environment definitions have too much repetition
  * variation can result in unexpected state
  * how can we ensure consistency between environments
Module dependencies are manageable bit it's the linkage between the networking layer and stacks that is brittle 
  * will stacks need to access each others state.
  * if there are variations between clouds then the pipeline can stop due to missing/incorrect state
Directory structure needs to support versioning
Will Git help?
  * without it any new modules added will require changes to the build system so that the ocrrect env deployment structure is created.
  * or use a manifest file to control this
    * patching would be to made a tag and the tag promoted to an environment
Refactoring is hard!
Need to be able to rapidly deploy but with safety
How to recover from failed apply actions?



Scenario - Failed apply due to duplicate named items - may not be a realsitic example

ran build/new-version-modules.sh to simulate modules job
export TESTCI-1 as this would be what would have been built by the modules ci job - V 1.0.0 of environments/TESTCI and V 1.0.0 of modules
init/plan/apply/destroy

This would be a good build so promote modules 1.0.0 to TESTC3


ran build/new-TESTC3-environment-and-modules.sh to simulate building TESTC3 with 1.0.0 of modules
ran build/promote-TESTC3-environment.sh to simulate promoting TESTC3 1.0.0 with 1.0.0 of modules to env deploy TESTC3-1
export TESTC3-1 to init/plan/apply

all good

ran build/new-TESTC4-environment-and-modules.sh to simulate building TESTC4 with 1.0.0 of modules
ran build/promote-TESTC4-environment.sh to simulate promoting TESTC4 1.0.0 with 1.0.0 of modules to env deploy TESTC4-1
export TESTC3-1 to init/plan/apply

Apply failed due to elb name duplicated




